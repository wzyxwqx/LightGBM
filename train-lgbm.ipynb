{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import lightgbm as lgb\n",
    "from sklearn import metrics\n",
    "import gc\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "root_path = '/kaggle/input/'\n",
    "import os\n",
    "#for dirname, _, filenames in os.walk(root_path):\n",
    "#    for filename in filenames:\n",
    "#        print(os.path.join(dirname, filename))\n",
    "mode = 'another_refit_test'\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ctypes\n",
    "from lightgbm import libpath as lbp\n",
    "find_lib_path = lbp.find_lib_path\n",
    "\n",
    "def _load_lib():\n",
    "    \"\"\"Load LightGBM library.\"\"\"\n",
    "    lib_path = find_lib_path()\n",
    "    if len(lib_path) == 0:\n",
    "        return None\n",
    "    lib = ctypes.cdll.LoadLibrary(lib_path[0])\n",
    "    lib.LGBM_GetLastError.restype = ctypes.c_char_p\n",
    "    return lib\n",
    "\n",
    "_LIB = _load_lib()\n",
    "\n",
    "def _safe_call(ret):\n",
    "    \"\"\"Check the return value from C API call.\n",
    "    Parameters\n",
    "    ----------\n",
    "    ret : int\n",
    "        The return value from C API calls.\n",
    "    \"\"\"\n",
    "    if ret != 0:\n",
    "        raise LightGBMError(decode_string(_LIB.LGBM_GetLastError()))\n",
    "\n",
    "def set_leaf_output(booster_obj, tree_id, leaf_id, value):\n",
    "    \"\"\"Get the output of a leaf.\n",
    "    Parameters\n",
    "    ----------\n",
    "    tree_id : int\n",
    "        The index of the tree.\n",
    "    leaf_id : int\n",
    "        The index of the leaf in the tree.\n",
    "    Returns\n",
    "    -------\n",
    "    result : float\n",
    "        The output of the leaf.\n",
    "    \"\"\"\n",
    "    ret = ctypes.c_double(0)\n",
    "    _safe_call(_LIB.LGBM_BoosterSetLeafValue(\n",
    "        booster_obj.handle,\n",
    "        ctypes.c_int(tree_id),\n",
    "        ctypes.c_int(leaf_id),\n",
    "        ctypes.c_double(value),\n",
    "        ctypes.byref(ret)))\n",
    "    return ret.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_tr_rows_cnt = 45840617 # rows in training set\n",
    "raw_ts_rows_cnt = 6042135  # rows in testing set\n",
    "I_col_names = ['I{}'.format(i) for i in range(1, 14)]\n",
    "C_col_names = ['C{}'.format(i) for i in range(1, 27)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = ['/kaggle/input/dataset-generator-10/train_{}_{}.csv'.format(i, i+10) for i in range(50, 91, 10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {'num_leaves': 128, \n",
    "             'num_trees': 64, \n",
    "             'objective': 'binary',\n",
    "             'metrics':['auc','binary_logloss'],\n",
    "             'learning_rate':0.3}\n",
    "name = '_'.join([str(param[k]) + (k) for k in ['num_leaves', 'num_trees', 'learning_rate']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_kg_hide-output": false
   },
   "outputs": [],
   "source": [
    "if mode == 'train':\n",
    "    lgb_train = lgb.Dataset('/kaggle/input/dataset-generator/train_0_50.csv',\n",
    "                            feature_name=I_col_names + C_col_names, \n",
    "                            categorical_feature=C_col_names,\n",
    "                            free_raw_data=False)\n",
    "    lgb_test = [lgb.Dataset(tp,\n",
    "                            reference=lgb_train) for tp in data_list[1:]]\n",
    "\n",
    "\n",
    "    gbm=lgb.train(param, lgb_train, valid_sets = lgb_test)\n",
    "    # this param performs best.\n",
    "    gbm_str = gbm.model_to_string()\n",
    "    with open('./{}.lgbm'.format(name),'wt') as f:\n",
    "        f.write(gbm_str)\n",
    "\n",
    "    del lgb_train\n",
    "    for _ in range(len(lgb_test)):\n",
    "        del lgb_test[0].data\n",
    "        del lgb_test[0]\n",
    "    gc.collect()\n",
    "    gbm.free_dataset()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if mode != 'train':\n",
    "    gbm = lgb.Booster(model_file=\"/kaggle/input/train-lgbm1/\"+name+\".lgbm\")\n",
    "    \"\"\"\n",
    "    Or you could simply achieve this by\n",
    "    lgbm_file = \"/kaggle/input/train-lgbm/128num_leaves_32num_trees_0.9learning_rate.lgbm\"\n",
    "    lgbm_str = \"\"\n",
    "    with open(lgbm_file,'r') as f:\n",
    "        lgbm_str = f.read()\n",
    "    gbm = lgb.Booster(model_str=lgbm_str)\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code block is for some API test\n",
    "# gbm_json = gbm.dump_model()\n",
    "# print(len(gbm_json['tree_info']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Baseline of refit function\n",
    "temp_gbm = gbm\n",
    "if mode == \"baseline_refit\":\n",
    "    for ind, (train_data, eval_data) in enumerate(zip(data_list[:-1],data_list[1:])):\n",
    "        print('use',train_data.split('/')[-1],'for training,', eval_data.split('/')[-1], 'for evaluation')\n",
    "        nx = pd.read_csv(train_data,\n",
    "                        names=['label']+I_col_names + C_col_names,\n",
    "                        )\n",
    "        temp_gbm = temp_gbm.refit(nx.drop(['label'],axis=1), nx['label'],decay_rate= 0.9)\n",
    "        del nx\n",
    "        gc.collect()\n",
    "        gt = pd.read_csv(eval_data,\n",
    "                        names=['label']+I_col_names + C_col_names)\n",
    "        pred = temp_gbm.predict(gt.drop(['label'],axis = 1))\n",
    "        print(metrics.log_loss(gt['label'], pred),'log_loss')\n",
    "        print(metrics.roc_auc_score(gt['label'], pred),'auc')\n",
    "        del gt\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm_list = [gbm]\n",
    "if mode == \"another_refit\":\n",
    "    for ind, (train_data, eval_data) in enumerate(zip(data_list[:-1],data_list[1:])):\n",
    "        print('use',train_data.split('/')[-1],'for training,', eval_data.split('/')[-1], 'for evaluation')\n",
    "        #nx = pd.read_csv(train_data,\n",
    "        #                names=['label']+I_col_names + C_col_names,\n",
    "        #                )\n",
    "        #temp_gbm = temp_gbm.refit(nx.drop(['label'],axis=1), nx['label'],decay_rate= 0.9)\n",
    "        #del nx\n",
    "        new_train = lgb.Dataset(train_data, feature_name=I_col_names + C_col_names, categorical_feature=C_col_names, free_raw_data=False)\n",
    "        new_val = lgb.Dataset(eval_data, reference=new_train)\n",
    "        gbm_new = lgb.train(param, new_train, valid_sets=new_val)\n",
    "        gbm_list.append(gbm_new)\n",
    "        gc.collect()\n",
    "        gt = pd.read_csv(eval_data,\n",
    "                        names=['label']+I_col_names + C_col_names)\n",
    "        pred = 0.5 * gbm_list[0].predict(gt.drop(['label'],axis = 1))\n",
    "        for k, gbm_item in enumerate(gbm_list[1:]):\n",
    "            pred = pred + 0.1 * gbm_item.predict(gt.drop(['label'],axis = 1))\n",
    "        pred = pred / (0.5 + 0.1 * ind)\n",
    "        print(metrics.log_loss(gt['label'], pred),'log_loss')\n",
    "        print(metrics.roc_auc_score(gt['label'], pred),'auc')\n",
    "        del gt\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use train_50_60.csv for training, train_60_70.csv for evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/lightgbm/engine.py:148: UserWarning: Found `num_trees` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/opt/conda/lib/python3.6/site-packages/lightgbm/basic.py:1243: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n",
      "/opt/conda/lib/python3.6/site-packages/lightgbm/basic.py:1247: UserWarning: categorical_feature in Dataset is overridden.\n",
      "New categorical_feature is ['C1', 'C10', 'C11', 'C12', 'C13', 'C14', 'C15', 'C16', 'C17', 'C18', 'C19', 'C2', 'C20', 'C21', 'C22', 'C23', 'C24', 'C25', 'C26', 'C3', 'C4', 'C5', 'C6', 'C7', 'C8', 'C9']\n",
      "  'New categorical_feature is {}'.format(sorted(list(categorical_feature))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\tvalid_0's auc: 0.741819\tvalid_0's binary_logloss: 0.534711\n",
      "[2]\tvalid_0's auc: 0.750773\tvalid_0's binary_logloss: 0.515609\n",
      "[3]\tvalid_0's auc: 0.757307\tvalid_0's binary_logloss: 0.503652\n",
      "[4]\tvalid_0's auc: 0.761909\tvalid_0's binary_logloss: 0.495659\n",
      "[5]\tvalid_0's auc: 0.766147\tvalid_0's binary_logloss: 0.489593\n",
      "[6]\tvalid_0's auc: 0.76908\tvalid_0's binary_logloss: 0.485385\n",
      "[7]\tvalid_0's auc: 0.771969\tvalid_0's binary_logloss: 0.481881\n",
      "[8]\tvalid_0's auc: 0.774313\tvalid_0's binary_logloss: 0.479136\n",
      "[9]\tvalid_0's auc: 0.777105\tvalid_0's binary_logloss: 0.476342\n",
      "[10]\tvalid_0's auc: 0.779307\tvalid_0's binary_logloss: 0.474206\n",
      "[11]\tvalid_0's auc: 0.780915\tvalid_0's binary_logloss: 0.47253\n",
      "[12]\tvalid_0's auc: 0.783147\tvalid_0's binary_logloss: 0.470589\n",
      "[13]\tvalid_0's auc: 0.784379\tvalid_0's binary_logloss: 0.469395\n",
      "[14]\tvalid_0's auc: 0.785222\tvalid_0's binary_logloss: 0.468492\n",
      "[15]\tvalid_0's auc: 0.786669\tvalid_0's binary_logloss: 0.467244\n",
      "[16]\tvalid_0's auc: 0.787639\tvalid_0's binary_logloss: 0.466316\n",
      "[17]\tvalid_0's auc: 0.78819\tvalid_0's binary_logloss: 0.465761\n",
      "[18]\tvalid_0's auc: 0.78905\tvalid_0's binary_logloss: 0.46503\n",
      "[19]\tvalid_0's auc: 0.789855\tvalid_0's binary_logloss: 0.464303\n",
      "[20]\tvalid_0's auc: 0.790409\tvalid_0's binary_logloss: 0.463797\n",
      "[21]\tvalid_0's auc: 0.791026\tvalid_0's binary_logloss: 0.46324\n",
      "[22]\tvalid_0's auc: 0.791372\tvalid_0's binary_logloss: 0.462912\n",
      "[23]\tvalid_0's auc: 0.791658\tvalid_0's binary_logloss: 0.462643\n",
      "[24]\tvalid_0's auc: 0.791907\tvalid_0's binary_logloss: 0.462417\n",
      "[25]\tvalid_0's auc: 0.792146\tvalid_0's binary_logloss: 0.462188\n",
      "[26]\tvalid_0's auc: 0.792337\tvalid_0's binary_logloss: 0.462025\n",
      "[27]\tvalid_0's auc: 0.792518\tvalid_0's binary_logloss: 0.461855\n",
      "[28]\tvalid_0's auc: 0.792646\tvalid_0's binary_logloss: 0.461735\n",
      "[29]\tvalid_0's auc: 0.792817\tvalid_0's binary_logloss: 0.461558\n",
      "[30]\tvalid_0's auc: 0.793107\tvalid_0's binary_logloss: 0.461327\n",
      "[31]\tvalid_0's auc: 0.793216\tvalid_0's binary_logloss: 0.461221\n",
      "[32]\tvalid_0's auc: 0.793289\tvalid_0's binary_logloss: 0.461157\n",
      "[33]\tvalid_0's auc: 0.793342\tvalid_0's binary_logloss: 0.461112\n",
      "[34]\tvalid_0's auc: 0.793593\tvalid_0's binary_logloss: 0.460909\n",
      "[35]\tvalid_0's auc: 0.793561\tvalid_0's binary_logloss: 0.460944\n",
      "[36]\tvalid_0's auc: 0.793685\tvalid_0's binary_logloss: 0.460852\n",
      "[37]\tvalid_0's auc: 0.793751\tvalid_0's binary_logloss: 0.460797\n",
      "[38]\tvalid_0's auc: 0.793786\tvalid_0's binary_logloss: 0.460746\n",
      "[39]\tvalid_0's auc: 0.793842\tvalid_0's binary_logloss: 0.460698\n",
      "[40]\tvalid_0's auc: 0.793889\tvalid_0's binary_logloss: 0.460668\n",
      "[41]\tvalid_0's auc: 0.793898\tvalid_0's binary_logloss: 0.460668\n",
      "[42]\tvalid_0's auc: 0.79392\tvalid_0's binary_logloss: 0.460663\n",
      "[43]\tvalid_0's auc: 0.794274\tvalid_0's binary_logloss: 0.460386\n",
      "[44]\tvalid_0's auc: 0.7944\tvalid_0's binary_logloss: 0.460305\n",
      "[45]\tvalid_0's auc: 0.794389\tvalid_0's binary_logloss: 0.460334\n",
      "[46]\tvalid_0's auc: 0.794352\tvalid_0's binary_logloss: 0.460371\n",
      "[47]\tvalid_0's auc: 0.794368\tvalid_0's binary_logloss: 0.460364\n",
      "[48]\tvalid_0's auc: 0.79437\tvalid_0's binary_logloss: 0.460375\n",
      "[49]\tvalid_0's auc: 0.794277\tvalid_0's binary_logloss: 0.46047\n",
      "[50]\tvalid_0's auc: 0.79428\tvalid_0's binary_logloss: 0.460479\n",
      "[51]\tvalid_0's auc: 0.794305\tvalid_0's binary_logloss: 0.460476\n",
      "[52]\tvalid_0's auc: 0.794358\tvalid_0's binary_logloss: 0.460469\n",
      "[53]\tvalid_0's auc: 0.794296\tvalid_0's binary_logloss: 0.460535\n",
      "[54]\tvalid_0's auc: 0.794311\tvalid_0's binary_logloss: 0.460539\n",
      "[55]\tvalid_0's auc: 0.794314\tvalid_0's binary_logloss: 0.460551\n",
      "[56]\tvalid_0's auc: 0.794253\tvalid_0's binary_logloss: 0.460613\n",
      "[57]\tvalid_0's auc: 0.794249\tvalid_0's binary_logloss: 0.460631\n",
      "[58]\tvalid_0's auc: 0.794206\tvalid_0's binary_logloss: 0.460685\n",
      "[59]\tvalid_0's auc: 0.794162\tvalid_0's binary_logloss: 0.460744\n",
      "[60]\tvalid_0's auc: 0.794181\tvalid_0's binary_logloss: 0.460747\n",
      "[61]\tvalid_0's auc: 0.794129\tvalid_0's binary_logloss: 0.460816\n",
      "[62]\tvalid_0's auc: 0.794127\tvalid_0's binary_logloss: 0.460827\n",
      "[63]\tvalid_0's auc: 0.794069\tvalid_0's binary_logloss: 0.460898\n",
      "[64]\tvalid_0's auc: 0.794025\tvalid_0's binary_logloss: 0.460946\n",
      "1.4982357839298146 log_loss\n",
      "0.802757835240937 auc\n",
      "use train_60_70.csv for training, train_70_80.csv for evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/lightgbm/engine.py:148: UserWarning: Found `num_trees` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/opt/conda/lib/python3.6/site-packages/lightgbm/basic.py:1243: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n",
      "/opt/conda/lib/python3.6/site-packages/lightgbm/basic.py:1247: UserWarning: categorical_feature in Dataset is overridden.\n",
      "New categorical_feature is ['C1', 'C10', 'C11', 'C12', 'C13', 'C14', 'C15', 'C16', 'C17', 'C18', 'C19', 'C2', 'C20', 'C21', 'C22', 'C23', 'C24', 'C25', 'C26', 'C3', 'C4', 'C5', 'C6', 'C7', 'C8', 'C9']\n",
      "  'New categorical_feature is {}'.format(sorted(list(categorical_feature))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\tvalid_0's auc: 0.736112\tvalid_0's binary_logloss: 0.537492\n",
      "[2]\tvalid_0's auc: 0.747742\tvalid_0's binary_logloss: 0.518303\n",
      "[3]\tvalid_0's auc: 0.753784\tvalid_0's binary_logloss: 0.506506\n",
      "[4]\tvalid_0's auc: 0.758455\tvalid_0's binary_logloss: 0.498555\n",
      "[5]\tvalid_0's auc: 0.762201\tvalid_0's binary_logloss: 0.492777\n",
      "[6]\tvalid_0's auc: 0.766365\tvalid_0's binary_logloss: 0.487938\n",
      "[7]\tvalid_0's auc: 0.76883\tvalid_0's binary_logloss: 0.484706\n",
      "[8]\tvalid_0's auc: 0.771468\tvalid_0's binary_logloss: 0.481753\n",
      "[9]\tvalid_0's auc: 0.77339\tvalid_0's binary_logloss: 0.479543\n",
      "[10]\tvalid_0's auc: 0.775071\tvalid_0's binary_logloss: 0.477716\n",
      "[11]\tvalid_0's auc: 0.777695\tvalid_0's binary_logloss: 0.475406\n",
      "[12]\tvalid_0's auc: 0.779141\tvalid_0's binary_logloss: 0.473969\n",
      "[13]\tvalid_0's auc: 0.780811\tvalid_0's binary_logloss: 0.472579\n",
      "[14]\tvalid_0's auc: 0.782006\tvalid_0's binary_logloss: 0.471429\n",
      "[15]\tvalid_0's auc: 0.782836\tvalid_0's binary_logloss: 0.470603\n",
      "[16]\tvalid_0's auc: 0.783476\tvalid_0's binary_logloss: 0.469942\n",
      "[17]\tvalid_0's auc: 0.784253\tvalid_0's binary_logloss: 0.469255\n",
      "[18]\tvalid_0's auc: 0.785373\tvalid_0's binary_logloss: 0.468306\n",
      "[19]\tvalid_0's auc: 0.785981\tvalid_0's binary_logloss: 0.467757\n",
      "[20]\tvalid_0's auc: 0.786922\tvalid_0's binary_logloss: 0.466979\n",
      "[21]\tvalid_0's auc: 0.787284\tvalid_0's binary_logloss: 0.466628\n",
      "[22]\tvalid_0's auc: 0.787733\tvalid_0's binary_logloss: 0.46623\n",
      "[23]\tvalid_0's auc: 0.788084\tvalid_0's binary_logloss: 0.4659\n",
      "[24]\tvalid_0's auc: 0.788731\tvalid_0's binary_logloss: 0.465378\n",
      "[25]\tvalid_0's auc: 0.788956\tvalid_0's binary_logloss: 0.465158\n",
      "[26]\tvalid_0's auc: 0.789236\tvalid_0's binary_logloss: 0.464914\n",
      "[27]\tvalid_0's auc: 0.789378\tvalid_0's binary_logloss: 0.464795\n",
      "[28]\tvalid_0's auc: 0.789694\tvalid_0's binary_logloss: 0.464529\n",
      "[29]\tvalid_0's auc: 0.789903\tvalid_0's binary_logloss: 0.464349\n",
      "[30]\tvalid_0's auc: 0.790153\tvalid_0's binary_logloss: 0.464151\n",
      "[31]\tvalid_0's auc: 0.790227\tvalid_0's binary_logloss: 0.464074\n",
      "[32]\tvalid_0's auc: 0.79037\tvalid_0's binary_logloss: 0.463958\n",
      "[33]\tvalid_0's auc: 0.790527\tvalid_0's binary_logloss: 0.463845\n",
      "[34]\tvalid_0's auc: 0.79058\tvalid_0's binary_logloss: 0.46381\n",
      "[35]\tvalid_0's auc: 0.790771\tvalid_0's binary_logloss: 0.463662\n",
      "[36]\tvalid_0's auc: 0.790808\tvalid_0's binary_logloss: 0.463643\n",
      "[37]\tvalid_0's auc: 0.790886\tvalid_0's binary_logloss: 0.463603\n",
      "[38]\tvalid_0's auc: 0.790955\tvalid_0's binary_logloss: 0.463564\n",
      "[39]\tvalid_0's auc: 0.791007\tvalid_0's binary_logloss: 0.463527\n",
      "[40]\tvalid_0's auc: 0.791013\tvalid_0's binary_logloss: 0.463532\n",
      "[41]\tvalid_0's auc: 0.791022\tvalid_0's binary_logloss: 0.463543\n",
      "[42]\tvalid_0's auc: 0.790995\tvalid_0's binary_logloss: 0.463584\n",
      "[43]\tvalid_0's auc: 0.791029\tvalid_0's binary_logloss: 0.463574\n",
      "[44]\tvalid_0's auc: 0.791073\tvalid_0's binary_logloss: 0.463565\n",
      "[45]\tvalid_0's auc: 0.791043\tvalid_0's binary_logloss: 0.463607\n",
      "[46]\tvalid_0's auc: 0.79106\tvalid_0's binary_logloss: 0.463608\n",
      "[47]\tvalid_0's auc: 0.7911\tvalid_0's binary_logloss: 0.463579\n",
      "[48]\tvalid_0's auc: 0.791169\tvalid_0's binary_logloss: 0.463545\n",
      "[49]\tvalid_0's auc: 0.791174\tvalid_0's binary_logloss: 0.46357\n",
      "[50]\tvalid_0's auc: 0.791171\tvalid_0's binary_logloss: 0.463587\n",
      "[51]\tvalid_0's auc: 0.791131\tvalid_0's binary_logloss: 0.463647\n",
      "[52]\tvalid_0's auc: 0.791113\tvalid_0's binary_logloss: 0.463673\n",
      "[53]\tvalid_0's auc: 0.791052\tvalid_0's binary_logloss: 0.463743\n",
      "[54]\tvalid_0's auc: 0.791014\tvalid_0's binary_logloss: 0.463788\n",
      "[55]\tvalid_0's auc: 0.790984\tvalid_0's binary_logloss: 0.463838\n",
      "[56]\tvalid_0's auc: 0.790975\tvalid_0's binary_logloss: 0.463861\n",
      "[57]\tvalid_0's auc: 0.790955\tvalid_0's binary_logloss: 0.463882\n",
      "[58]\tvalid_0's auc: 0.790918\tvalid_0's binary_logloss: 0.463939\n",
      "[59]\tvalid_0's auc: 0.790938\tvalid_0's binary_logloss: 0.463947\n",
      "[60]\tvalid_0's auc: 0.79088\tvalid_0's binary_logloss: 0.464021\n",
      "[61]\tvalid_0's auc: 0.790862\tvalid_0's binary_logloss: 0.464052\n",
      "[62]\tvalid_0's auc: 0.79081\tvalid_0's binary_logloss: 0.464109\n",
      "[63]\tvalid_0's auc: 0.790787\tvalid_0's binary_logloss: 0.464155\n",
      "[64]\tvalid_0's auc: 0.790776\tvalid_0's binary_logloss: 0.464182\n",
      "0.8025676720137794 log_loss\n",
      "0.8013626526158978 auc\n",
      "use train_70_80.csv for training, train_80_90.csv for evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/lightgbm/engine.py:148: UserWarning: Found `num_trees` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/opt/conda/lib/python3.6/site-packages/lightgbm/basic.py:1243: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n",
      "/opt/conda/lib/python3.6/site-packages/lightgbm/basic.py:1247: UserWarning: categorical_feature in Dataset is overridden.\n",
      "New categorical_feature is ['C1', 'C10', 'C11', 'C12', 'C13', 'C14', 'C15', 'C16', 'C17', 'C18', 'C19', 'C2', 'C20', 'C21', 'C22', 'C23', 'C24', 'C25', 'C26', 'C3', 'C4', 'C5', 'C6', 'C7', 'C8', 'C9']\n",
      "  'New categorical_feature is {}'.format(sorted(list(categorical_feature))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\tvalid_0's auc: 0.731928\tvalid_0's binary_logloss: 0.531414\n",
      "[2]\tvalid_0's auc: 0.744826\tvalid_0's binary_logloss: 0.512783\n",
      "[3]\tvalid_0's auc: 0.750625\tvalid_0's binary_logloss: 0.501545\n",
      "[4]\tvalid_0's auc: 0.755402\tvalid_0's binary_logloss: 0.493799\n",
      "[5]\tvalid_0's auc: 0.758848\tvalid_0's binary_logloss: 0.488295\n",
      "[6]\tvalid_0's auc: 0.76238\tvalid_0's binary_logloss: 0.483848\n",
      "[7]\tvalid_0's auc: 0.766115\tvalid_0's binary_logloss: 0.48002\n",
      "[8]\tvalid_0's auc: 0.768381\tvalid_0's binary_logloss: 0.477282\n",
      "[9]\tvalid_0's auc: 0.771352\tvalid_0's binary_logloss: 0.47433\n",
      "[10]\tvalid_0's auc: 0.773079\tvalid_0's binary_logloss: 0.472499\n",
      "[11]\tvalid_0's auc: 0.776109\tvalid_0's binary_logloss: 0.469997\n",
      "[12]\tvalid_0's auc: 0.777152\tvalid_0's binary_logloss: 0.468822\n",
      "[13]\tvalid_0's auc: 0.778719\tvalid_0's binary_logloss: 0.467504\n",
      "[14]\tvalid_0's auc: 0.779568\tvalid_0's binary_logloss: 0.46658\n",
      "[15]\tvalid_0's auc: 0.781205\tvalid_0's binary_logloss: 0.465163\n",
      "[16]\tvalid_0's auc: 0.781896\tvalid_0's binary_logloss: 0.464501\n",
      "[17]\tvalid_0's auc: 0.782969\tvalid_0's binary_logloss: 0.46362\n",
      "[18]\tvalid_0's auc: 0.784284\tvalid_0's binary_logloss: 0.462542\n",
      "[19]\tvalid_0's auc: 0.784786\tvalid_0's binary_logloss: 0.462072\n",
      "[20]\tvalid_0's auc: 0.785209\tvalid_0's binary_logloss: 0.461656\n",
      "[21]\tvalid_0's auc: 0.785701\tvalid_0's binary_logloss: 0.461252\n",
      "[22]\tvalid_0's auc: 0.786158\tvalid_0's binary_logloss: 0.460854\n",
      "[23]\tvalid_0's auc: 0.78684\tvalid_0's binary_logloss: 0.460298\n",
      "[24]\tvalid_0's auc: 0.78715\tvalid_0's binary_logloss: 0.460016\n",
      "[25]\tvalid_0's auc: 0.787596\tvalid_0's binary_logloss: 0.459642\n",
      "[26]\tvalid_0's auc: 0.787696\tvalid_0's binary_logloss: 0.459541\n",
      "[27]\tvalid_0's auc: 0.787874\tvalid_0's binary_logloss: 0.459381\n",
      "[28]\tvalid_0's auc: 0.788074\tvalid_0's binary_logloss: 0.459196\n",
      "[29]\tvalid_0's auc: 0.788224\tvalid_0's binary_logloss: 0.459045\n",
      "[30]\tvalid_0's auc: 0.788598\tvalid_0's binary_logloss: 0.458755\n",
      "[31]\tvalid_0's auc: 0.788864\tvalid_0's binary_logloss: 0.458547\n",
      "[32]\tvalid_0's auc: 0.789013\tvalid_0's binary_logloss: 0.458414\n",
      "[33]\tvalid_0's auc: 0.789135\tvalid_0's binary_logloss: 0.458311\n",
      "[34]\tvalid_0's auc: 0.78922\tvalid_0's binary_logloss: 0.458238\n",
      "[35]\tvalid_0's auc: 0.789296\tvalid_0's binary_logloss: 0.458174\n",
      "[36]\tvalid_0's auc: 0.78934\tvalid_0's binary_logloss: 0.458151\n",
      "[37]\tvalid_0's auc: 0.789336\tvalid_0's binary_logloss: 0.458168\n",
      "[38]\tvalid_0's auc: 0.789513\tvalid_0's binary_logloss: 0.45804\n",
      "[39]\tvalid_0's auc: 0.789518\tvalid_0's binary_logloss: 0.458046\n",
      "[40]\tvalid_0's auc: 0.789512\tvalid_0's binary_logloss: 0.458061\n",
      "[41]\tvalid_0's auc: 0.789458\tvalid_0's binary_logloss: 0.458119\n",
      "[42]\tvalid_0's auc: 0.78972\tvalid_0's binary_logloss: 0.457924\n",
      "[43]\tvalid_0's auc: 0.789725\tvalid_0's binary_logloss: 0.457928\n",
      "[44]\tvalid_0's auc: 0.78973\tvalid_0's binary_logloss: 0.457938\n",
      "[45]\tvalid_0's auc: 0.789662\tvalid_0's binary_logloss: 0.458\n",
      "[46]\tvalid_0's auc: 0.78962\tvalid_0's binary_logloss: 0.458052\n",
      "[47]\tvalid_0's auc: 0.78963\tvalid_0's binary_logloss: 0.458035\n",
      "[48]\tvalid_0's auc: 0.789847\tvalid_0's binary_logloss: 0.457888\n",
      "[49]\tvalid_0's auc: 0.789818\tvalid_0's binary_logloss: 0.457927\n",
      "[50]\tvalid_0's auc: 0.789759\tvalid_0's binary_logloss: 0.457995\n",
      "[51]\tvalid_0's auc: 0.789826\tvalid_0's binary_logloss: 0.457964\n",
      "[52]\tvalid_0's auc: 0.789824\tvalid_0's binary_logloss: 0.457987\n",
      "[53]\tvalid_0's auc: 0.789825\tvalid_0's binary_logloss: 0.457997\n",
      "[54]\tvalid_0's auc: 0.789782\tvalid_0's binary_logloss: 0.458058\n",
      "[55]\tvalid_0's auc: 0.789778\tvalid_0's binary_logloss: 0.458079\n",
      "[56]\tvalid_0's auc: 0.789865\tvalid_0's binary_logloss: 0.458026\n",
      "[57]\tvalid_0's auc: 0.789805\tvalid_0's binary_logloss: 0.458091\n",
      "[58]\tvalid_0's auc: 0.789785\tvalid_0's binary_logloss: 0.458125\n",
      "[59]\tvalid_0's auc: 0.789746\tvalid_0's binary_logloss: 0.458184\n",
      "[60]\tvalid_0's auc: 0.789656\tvalid_0's binary_logloss: 0.458271\n",
      "[61]\tvalid_0's auc: 0.789585\tvalid_0's binary_logloss: 0.45835\n",
      "[62]\tvalid_0's auc: 0.789601\tvalid_0's binary_logloss: 0.458359\n",
      "[63]\tvalid_0's auc: 0.789546\tvalid_0's binary_logloss: 0.458426\n",
      "[64]\tvalid_0's auc: 0.789506\tvalid_0's binary_logloss: 0.458479\n",
      "0.5743331104014083 log_loss\n",
      "0.8004027366601724 auc\n",
      "use train_80_90.csv for training, train_90_100.csv for evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/lightgbm/engine.py:148: UserWarning: Found `num_trees` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/opt/conda/lib/python3.6/site-packages/lightgbm/basic.py:1243: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n",
      "/opt/conda/lib/python3.6/site-packages/lightgbm/basic.py:1247: UserWarning: categorical_feature in Dataset is overridden.\n",
      "New categorical_feature is ['C1', 'C10', 'C11', 'C12', 'C13', 'C14', 'C15', 'C16', 'C17', 'C18', 'C19', 'C2', 'C20', 'C21', 'C22', 'C23', 'C24', 'C25', 'C26', 'C3', 'C4', 'C5', 'C6', 'C7', 'C8', 'C9']\n",
      "  'New categorical_feature is {}'.format(sorted(list(categorical_feature))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\tvalid_0's auc: 0.733739\tvalid_0's binary_logloss: 0.535278\n",
      "[2]\tvalid_0's auc: 0.74665\tvalid_0's binary_logloss: 0.516193\n",
      "[3]\tvalid_0's auc: 0.751573\tvalid_0's binary_logloss: 0.505209\n",
      "[4]\tvalid_0's auc: 0.755737\tvalid_0's binary_logloss: 0.497775\n",
      "[5]\tvalid_0's auc: 0.759991\tvalid_0's binary_logloss: 0.491808\n",
      "[6]\tvalid_0's auc: 0.762858\tvalid_0's binary_logloss: 0.487618\n",
      "[7]\tvalid_0's auc: 0.765807\tvalid_0's binary_logloss: 0.484133\n",
      "[8]\tvalid_0's auc: 0.767729\tvalid_0's binary_logloss: 0.481716\n",
      "[9]\tvalid_0's auc: 0.770738\tvalid_0's binary_logloss: 0.478853\n",
      "[10]\tvalid_0's auc: 0.772903\tvalid_0's binary_logloss: 0.47679\n",
      "[11]\tvalid_0's auc: 0.775703\tvalid_0's binary_logloss: 0.474357\n",
      "[12]\tvalid_0's auc: 0.776846\tvalid_0's binary_logloss: 0.47316\n",
      "[13]\tvalid_0's auc: 0.778794\tvalid_0's binary_logloss: 0.471506\n",
      "[14]\tvalid_0's auc: 0.779691\tvalid_0's binary_logloss: 0.470593\n",
      "[15]\tvalid_0's auc: 0.780805\tvalid_0's binary_logloss: 0.469626\n",
      "[16]\tvalid_0's auc: 0.781805\tvalid_0's binary_logloss: 0.468742\n",
      "[17]\tvalid_0's auc: 0.782418\tvalid_0's binary_logloss: 0.468165\n",
      "[18]\tvalid_0's auc: 0.782945\tvalid_0's binary_logloss: 0.467659\n",
      "[19]\tvalid_0's auc: 0.783658\tvalid_0's binary_logloss: 0.467039\n",
      "[20]\tvalid_0's auc: 0.784483\tvalid_0's binary_logloss: 0.466356\n",
      "[21]\tvalid_0's auc: 0.784848\tvalid_0's binary_logloss: 0.466021\n",
      "[22]\tvalid_0's auc: 0.785381\tvalid_0's binary_logloss: 0.465568\n",
      "[23]\tvalid_0's auc: 0.78568\tvalid_0's binary_logloss: 0.465295\n",
      "[24]\tvalid_0's auc: 0.785877\tvalid_0's binary_logloss: 0.465099\n",
      "[25]\tvalid_0's auc: 0.786543\tvalid_0's binary_logloss: 0.464569\n",
      "[26]\tvalid_0's auc: 0.786799\tvalid_0's binary_logloss: 0.464349\n",
      "[27]\tvalid_0's auc: 0.786991\tvalid_0's binary_logloss: 0.464189\n",
      "[28]\tvalid_0's auc: 0.787146\tvalid_0's binary_logloss: 0.464048\n",
      "[29]\tvalid_0's auc: 0.787356\tvalid_0's binary_logloss: 0.463872\n",
      "[30]\tvalid_0's auc: 0.787507\tvalid_0's binary_logloss: 0.463725\n",
      "[31]\tvalid_0's auc: 0.787693\tvalid_0's binary_logloss: 0.463573\n",
      "[32]\tvalid_0's auc: 0.787906\tvalid_0's binary_logloss: 0.463402\n",
      "[33]\tvalid_0's auc: 0.787948\tvalid_0's binary_logloss: 0.463378\n",
      "[34]\tvalid_0's auc: 0.788022\tvalid_0's binary_logloss: 0.463317\n",
      "[35]\tvalid_0's auc: 0.788042\tvalid_0's binary_logloss: 0.463306\n",
      "[36]\tvalid_0's auc: 0.788028\tvalid_0's binary_logloss: 0.463324\n",
      "[37]\tvalid_0's auc: 0.788066\tvalid_0's binary_logloss: 0.463309\n",
      "[38]\tvalid_0's auc: 0.78816\tvalid_0's binary_logloss: 0.463231\n",
      "[39]\tvalid_0's auc: 0.788243\tvalid_0's binary_logloss: 0.463175\n",
      "[40]\tvalid_0's auc: 0.788355\tvalid_0's binary_logloss: 0.463097\n",
      "[41]\tvalid_0's auc: 0.788322\tvalid_0's binary_logloss: 0.463135\n",
      "[42]\tvalid_0's auc: 0.788317\tvalid_0's binary_logloss: 0.463148\n",
      "[43]\tvalid_0's auc: 0.788291\tvalid_0's binary_logloss: 0.463184\n",
      "[44]\tvalid_0's auc: 0.788286\tvalid_0's binary_logloss: 0.463205\n",
      "[45]\tvalid_0's auc: 0.788252\tvalid_0's binary_logloss: 0.463253\n",
      "[46]\tvalid_0's auc: 0.788267\tvalid_0's binary_logloss: 0.463257\n",
      "[47]\tvalid_0's auc: 0.788247\tvalid_0's binary_logloss: 0.463286\n",
      "[48]\tvalid_0's auc: 0.788235\tvalid_0's binary_logloss: 0.463308\n",
      "[49]\tvalid_0's auc: 0.788173\tvalid_0's binary_logloss: 0.463376\n",
      "[50]\tvalid_0's auc: 0.788216\tvalid_0's binary_logloss: 0.463353\n",
      "[51]\tvalid_0's auc: 0.788303\tvalid_0's binary_logloss: 0.46331\n",
      "[52]\tvalid_0's auc: 0.788278\tvalid_0's binary_logloss: 0.463351\n",
      "[53]\tvalid_0's auc: 0.788283\tvalid_0's binary_logloss: 0.463364\n",
      "[54]\tvalid_0's auc: 0.788267\tvalid_0's binary_logloss: 0.463391\n",
      "[55]\tvalid_0's auc: 0.78824\tvalid_0's binary_logloss: 0.463425\n",
      "[56]\tvalid_0's auc: 0.788213\tvalid_0's binary_logloss: 0.463467\n",
      "[57]\tvalid_0's auc: 0.788168\tvalid_0's binary_logloss: 0.463529\n",
      "[58]\tvalid_0's auc: 0.788106\tvalid_0's binary_logloss: 0.463607\n",
      "[59]\tvalid_0's auc: 0.788129\tvalid_0's binary_logloss: 0.463602\n",
      "[60]\tvalid_0's auc: 0.788098\tvalid_0's binary_logloss: 0.463644\n",
      "[61]\tvalid_0's auc: 0.788052\tvalid_0's binary_logloss: 0.463703\n",
      "[62]\tvalid_0's auc: 0.787989\tvalid_0's binary_logloss: 0.463769\n",
      "[63]\tvalid_0's auc: 0.787981\tvalid_0's binary_logloss: 0.463788\n",
      "[64]\tvalid_0's auc: 0.787955\tvalid_0's binary_logloss: 0.463824\n",
      "0.5164571420816949 log_loss\n",
      "0.7994186687672978 auc\n"
     ]
    }
   ],
   "source": [
    "gbm_list = [gbm]\n",
    "alpha = 2.5\n",
    "if mode == \"RBM\":\n",
    "    for ind, (train_data, eval_data) in enumerate(zip(data_list[:-1],data_list[1:])):\n",
    "        print('use',train_data.split('/')[-1],'for training,', eval_data.split('/')[-1], 'for evaluation')\n",
    "        #nx = pd.read_csv(train_data,\n",
    "        #                names=['label']+I_col_names + C_col_names,\n",
    "        #                )\n",
    "        #temp_gbm = temp_gbm.refit(nx.drop(['label'],axis=1), nx['label'],decay_rate= 0.9)\n",
    "        #del nx\n",
    "        new_train = lgb.Dataset(train_data, feature_name=I_col_names + C_col_names, categorical_feature=C_col_names, free_raw_data=False)\n",
    "        new_val = lgb.Dataset(eval_data, reference=new_train)\n",
    "        gbm_new = lgb.train(param, new_train, valid_sets=new_val)\n",
    "        \n",
    "        gt = pd.read_csv(eval_data,\n",
    "                        names=['label']+I_col_names + C_col_names)\n",
    "        pred = 0.5 * gbm_list[0].predict(gt.drop(['label'],axis = 1))\n",
    "        for k, gbm_item in enumerate(gbm_list[1:]):\n",
    "            pred = pred + alpha * 0.1 * gbm_item.predict(gt.drop(['label'],axis = 1))\n",
    "            \n",
    "        pred = pred + 0.1 * gbm_new.predict(gt.drop(['label'],axis = 1))\n",
    "        gbm_list.append(gbm_new)\n",
    "        gc.collect()\n",
    "        pred = pred / (0.5 + 0.1*alpha*(ind-1) + 0.1)\n",
    "        print(metrics.log_loss(gt['label'], pred),'log_loss')\n",
    "        print(metrics.roc_auc_score(gt['label'], pred),'auc')\n",
    "        del gt\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### refit network test\n",
    "if mode == 'SIR':\n",
    "    for ind, (train_data, eval_data) in enumerate(zip(data_list[:-1],data_list[1:])):\n",
    "        print('use',train_data.split('/')[-1],'for training,', eval_data.split('/')[-1], 'for evaluation')\n",
    "        nx = pd.read_csv(train_data,\n",
    "                        names=['label']+I_col_names + C_col_names,\n",
    "                        )\n",
    "        gbm.refit_threshold(nx.drop(['label'],axis=1), nx['label'],decay_rate= 0)\n",
    "        refit_new_booster = gbm.refit(nx.drop(['label'],axis=1), nx['label'],decay_rate= 0)\n",
    "        del nx\n",
    "        gbm_json = gbm.dump_model()\n",
    "        gbm_string = gbm.model_to_string()\n",
    "        # here rnb stands for refit_new_booster\n",
    "        rnb_string = refit_new_booster.model_to_string()\n",
    "        tree_struct = gbm_json['tree_info']\n",
    "        tree_number = len(tree_struct)\n",
    "        gbm_leaves_cnt = np.zeros((tree_number, param['num_leaves']), dtype=np.uint32)\n",
    "        refit_leaves_cnt = np.zeros((tree_number, param['num_leaves']), dtype=np.uint32)\n",
    "        # These codes here are very dirty\n",
    "        for i in range(0,2):\n",
    "            tree_index = -1\n",
    "            booster_string = rnb_string if i > 0 else gbm_string\n",
    "            for bis in booster_string.split('\\n'):\n",
    "                # bis: booster_info_string\n",
    "                if bis[0:len(\"Tree\")]==\"Tree\":\n",
    "                    # record which tree is being dealt with\n",
    "                    tree_index += 1\n",
    "                if bis[0:len(\"leaf_count\")]==\"leaf_count\":\n",
    "                    # This code block could be \"simplified\" with reg expressions\n",
    "                    leaf_cnt = bis.split(\"=\")\n",
    "                    leaf_cnt = leaf_cnt[1]\n",
    "                    leaf_cnt = leaf_cnt.split(\" \")\n",
    "                    for leaf_index, element in enumerate(leaf_cnt):\n",
    "                        if i:\n",
    "                            refit_leaves_cnt[tree_index, leaf_index] = np.uint32(element)\n",
    "                        else:\n",
    "                            gbm_leaves_cnt[tree_index, leaf_index] = np.uint32(element)\n",
    "\n",
    "        # We still need a leafcnt matrix of the original data set\n",
    "        # Since the newly obtained refit booster has the same structure as the original booster\n",
    "        for treeId in range(0, tree_number):\n",
    "            for leafId in range(0, tree_struct[treeId]['num_leaves']):\n",
    "                gbm_leaf_cnt = gbm_leaves_cnt[treeId, leafId]\n",
    "                refit_leaf_cnt = refit_leaves_cnt[treeId, leafId]\n",
    "                gbm_leaf_output = gbm.get_leaf_output(treeId, leafId)\n",
    "                refit_leaf_output = refit_new_booster.get_leaf_output(treeId, leafId)\n",
    "                new_output = gbm_leaf_cnt / (gbm_leaf_cnt + refit_leaf_cnt) * gbm_leaf_output + refit_leaf_cnt / (gbm_leaf_cnt + refit_leaf_cnt) * refit_leaf_output\n",
    "                set_leaf_output(gbm, treeId, leafId, new_output)\n",
    "    #    del nx\n",
    "        gc.collect()\n",
    "        gt = pd.read_csv(eval_data,\n",
    "                        names=['label']+I_col_names + C_col_names)\n",
    "        pred = gbm.predict(gt.drop(['label'],axis = 1))\n",
    "        print(metrics.log_loss(gt['label'], pred),'log_loss')\n",
    "        print(metrics.roc_auc_score(gt['label'], pred),'auc')\n",
    "        del gt\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if mode == \"raw_test\":\n",
    "    for ind, (train_data, eval_data) in enumerate(zip(data_list[:-1],data_list[1:])):\n",
    "        print('use', eval_data.split('/')[-1], 'for evaluation')\n",
    "        gc.collect()\n",
    "        gt = pd.read_csv(eval_data,\n",
    "                        names=['label']+I_col_names + C_col_names)\n",
    "        pred = gbm.predict(gt.drop(['label'],axis = 1))\n",
    "        print(metrics.log_loss(gt['label'], pred),'log_loss')\n",
    "        print(metrics.roc_auc_score(gt['label'], pred),'auc')\n",
    "        del gt\n",
    "        gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
